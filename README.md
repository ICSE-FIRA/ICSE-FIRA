# FIRA: Fine-Grained Graph-Based Code Change Representation for Automated Commit Message Generation
FIRA is a learning-based commit message generation approach, which first represents code changes via fine-grained graphs and then learns to generate commit messages automatically. In this repository, we provide our code and the data we use.

## 1 Environment
+ Python == 3.8.5
+ Pytorch == 1.7.1
+ Numpy == 1.19.2
+ Scipy == 1.5.4
+ Nltk == 3.5
+ Sacrebleu == 1.5.1
+ Sumeval == 0.2.2

## 2 Preprocess

The folder `Preprocess` contains the scrips to preprocess data to get **abstrat syntax tree (AST)**, **edit operation** and **edge**. You can enter the directory `./Preprocess`  

```shell
cd ./Preprocess
```

and run

```
python run_total_process_data.py num_processes num_tasks
```
to preprocess the data and run 
```
python gather_data.py
```
to gather the data and the final **AST**, **edit operation** and **edge** will be put in the folder `DataSet`. We use `subprocess` module of `python` to preprocess parallelly. The arguments `num_processes` and `num_tasks` are the number of parallel subprocesses and the number of tasks one subprocess executes. The two arguments should be set according to the capacity of the CPU.
## 3 Dataset

The folder `DataSet` contains all the data which was already preprocessed, and can be directly used to train or evaluate the model. The descripstion of each file is as follows.

The following files are the original files of the [benchmark](https://github.com/SoftWiser-group/CoDiSum/blob/master/data4CopynetV3.zip), which is well established by Xu et al. [1].

+ difftoken.json: tokens of code changes
+ diffmark.json: marks indicating each token is added, deleted or unchanged 
+ diffatt.json: sub-tokens of each token
+ variable.json: mapping between placeholders and identifiers
+ msg.json: tokens of commit messages
+ word_vocab.json: vocabulary of code changes and commit messages

The following files are got by our preprocessing scrips, which is already introduced in  section **Preprocess**.

+ ast.json: abstrat syntax trees of code changes
+ change.json: edit operations between old code and new code
+ ast_change_vocab.json: vocabulary of AST and edit operations
+ edge_ast.json: edges of AST
+ edge_ast_code.json: edges between AST and tokens of code changes 
+ edge_change_ast.json: edges between edit operations and AST
+ edge_change_code.json: edges between edit operations and tokens of code changes

The files in `DataSet` will be read by **Dataset.py**, and converted to tensors and fed to the model. 

## 4 Model

We use GNN as encoder and transformer with dual copy mechanism as decoder. We define the model in file `Model.py`. If you want to train the model, you can run
```
python ./run_model.py train
```
and the model will be saved as `best_model.pt`. Because of the size limit of files uploaded to GitHub, we put `best_model.pt`,  which is already trained,  in our [zenodo repository](https://zenodo.org/record/5915220). 

If you want to evaluate the model, you can run
```
python ./run_model.py test
```
and the output commit messages will be saved in `OUTPUT/output_fira`.
## 5 Output
The folder `OUTPUT` contains the commit messages generated by FIRA and other compared approaches.
## 6 Metrics
The folder `Metrics` contains the scripts to compute the metrics we use to evaluate our approach, including BLEU, ROUGE-L, METEOR, and Penalty-BLEU. The commands to execute are as follows, and `ref` is the ground_truth commit message and `gen` is the generated commit message. 

`Bleu-B-Norm.py`, `Rouge.py`, and `Meteor.py` are from [the scripts provided by Tao et al. [2]](https://github.com/DeepSoftwareAnalytics/CommitMsgEmpirical/tree/main/metrics), who conducted an experimental study on the evaluation of commit message generation models and found that B-Norm BLEU exhibits the most consistently with human judgements on the quality of commit messages.

```
python ./Metrics/Bleu-B-Norm.py ref < gen

python ./Metrics/Rouge.py --ref_path ref --gen_path gen

python ./Metrics/Meteor.py --ref_path ref --gen_path gen

python ./Metrics/Bleu-Penalty.py ref < gen
```
## 7 Human Evaluation
The folder `HumanEvaluation` contains the scores of the six participants.

##  8 Docker Image

We also provide a [Docker image](https://hub.docker.com/layers/djhao/icse_docker/newest/images/sha256-374381239d18ec1db1a5697f72864e2f8ddc9e28a7bc92df6439ed235da75caa) to reproduce the results in our paper.

### 8.1 Requirements

+ Docker

+ NVIDIA GPU

+ NVIDIA Linux driver

+ NVIDIA Docker



+ It is worth noting that, our artifact can also reproduce the results in the environment without **NVIDIA GPU**, **NVIDIA driver** and **NVIDIA Docker**, but it will cost more time.

### 8.2 Preparation

+ Pull the [image](https://hub.docker.com/layers/djhao/icse_docker/newest/images/sha256-374381239d18ec1db1a5697f72864e2f8ddc9e28a7bc92df6439ed235da75caa)

```shell
$ docker pull djhao/icse_docker:newest
```

+ Run the image as a container using all GPUs

```shell
$ docker run -it --gpus all djhao/icse_docker:newest
```

or using the specified GPUs, such as 0, 1 in the following command

```shell
$ docker run -it --gpus '"device=0,1"' djhao/icse_docker:newest
```

+ If you don't have GPUs, our artifact also support to be executed on only CPU, you can run

```shell
$ docker run -it djhao/icse_docker:newest
```

to enter the CPU environment.

### 8.3 Reproduction

After running the container, you will enter the folder **/icse_final**, and you can run the following commands in the container to reproduce the results. 

#### 8.3.1 Training and inference of the model

A neural network model includes two process, training and inference. To train our model, you can run

```shell
$ python run_model.py train
```

However, the training time is too long, so we provide the model which is trained already in **best_model.pt**. With **best_model.pt**, you can directly evaluate the model and get the final output by executing

```shell
$ python run_model.py test
```

and the output will be saved as OUTPUT/output_fira. The inference process will cost around 6 minutes on one NVIDIA GeForce RTX 3090.

#### 8.3.2 Getting results of the paper

We use **table1.py**, **table2.py**, **table3.py**, **table4.py**, **table6.py** to get the results in **Table1-4** and **Table6**. We put all the scripts, that is, **table1-4.py** and **table6.py** in one script **runtotal.sh**. You can run **runtotal.sh** and get the results in all tables of the paper, and the execution will cost around 3 minutes.

```shell
$ bash runtotal.sh
```

The results will be saved in **RESULTS/table1-4,6.txt**, which is in the same format as that in the paper.

## 9 Reference

[1] Xu S, Yao Y, Xu F, et al. Commit message generation for source code changes[C]//IJCAI. 2019.0

[2] Tao W, Wang Y, Shi E, et al. On the Evaluation of Commit Message Generation Models: An Experimental Study[C]//2021 IEEE International Conference on Software Maintenance and Evolution (ICSME). IEEE, 2021: 126-136.
